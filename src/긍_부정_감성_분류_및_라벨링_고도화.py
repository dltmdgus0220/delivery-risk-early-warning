# -*- coding: utf-8 -*-
"""긍/부정 감성 분류 및 라벨링 고도화.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IDdnX2-s4pv4aSwL8dvsGcOFmWh10G2b
"""

!pip install koreanize-matplotlib

import os
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from typing import List
import seaborn as sns
import koreanize_matplotlib
import torch
from datasets import Dataset as HFDataset
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW # BERT에서 거의 표준으로 사용하는 옵티마이저

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
from sklearn.metrics import precision_recall_curve
from torch.nn.functional import softmax
import torch.nn.functional as F

from transformers import AutoTokenizer, AutoModelForSequenceClassification,TrainingArguments, Trainer
from tqdm import tqdm

"""# 감성 분류 모델링"""

df = pd.read_csv(r"/content/drive/MyDrive/data/baemin_reviews_playstore_100000.csv")

df = df[["content", "sentiment_label","score",'at']].dropna().copy()

# -1, 0, 1 → 0,1,2
if df["sentiment_label"].min() == -1:
    df["sentiment_label"] += 1


# 0(부정), 2(긍정)만 사용
df = df[df["sentiment_label"] != 1]
df["sentiment_label"] = df["sentiment_label"].map({0:0, 2:1})   # binary, 다시 0,1로 만듦

df.head()

train_df, test_df = train_test_split(
    df, test_size=0.2, stratify=df["sentiment_label"], random_state=42
)

model_name = "kykim/bert-kor-base"
tokenizer = AutoTokenizer.from_pretrained(model_name)

def tokenize(batch):
    return tokenizer(batch["content"], truncation=True, padding="max_length", max_length=96)

train_ds = HFDataset.from_pandas(train_df)
test_ds  = HFDataset.from_pandas(test_df)

train_ds = train_ds.map(tokenize, batched=True)
test_ds  = test_ds.map(tokenize, batched=True)

train_ds = train_ds.rename_column("sentiment_label", "labels")
test_ds  = test_ds.rename_column("sentiment_label", "labels")

train_ds.set_format("torch", columns=["input_ids","attention_mask","labels"])
test_ds.set_format("torch", columns=["input_ids","attention_mask","labels"])

# 1차 LMKor-BERT
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

def compute_metrics(pred):
    labels = pred.label_ids
    preds = np.argmax(pred.predictions, axis=1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1": f1_score(labels, preds)
    }

args = TrainingArguments(
    output_dir="./stage1",
    eval_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    num_train_epochs=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    load_best_model_at_end=True
)

trainer1 = Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics
)

trainer1.train()

"""재라벨링 코드"""

preds = trainer1.predict(train_ds)
probs = softmax(torch.tensor(preds.predictions), dim=1).numpy()
labels = preds.label_ids

neg_conf = probs[:, 0]
pos_conf = probs[:, 1]

threshold = 0.8
neutral_zone = (0.4, 0.6)
new_labels = labels.copy()

# 중립(양가감정) 여부를 저장할 리스트
ambivalent_indices = []

for i in range(len(labels)):
    if labels[i] == 0 and pos_conf[i] >= threshold:
        new_labels[i] = 1
    elif labels[i] == 1 and neg_conf[i] >= threshold:
        new_labels[i] = 0

    # 양가감정 데이터 기록
    if neutral_zone[0] <= pos_conf[i] <= neutral_zone[1]:
        ambivalent_indices.append(True)
    else:
        ambivalent_indices.append(False)

print("재라벨링 비율:", np.mean(new_labels != labels))

train_df2 = train_ds.to_pandas()
train_df2["labels"] = new_labels

train_ds2 = HFDataset.from_pandas(train_df2)
train_ds2 = train_ds2.map(tokenize, batched=True)
train_ds2.set_format("torch", columns=["input_ids","attention_mask","labels"])


print("새로운 라벨 분포:\n", train_df2["labels"].value_counts())

# 2차 LMKor-BERT
model2 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

trainer2 = Trainer(
    model=model2,
    args=args,
    train_dataset=train_ds2,
    eval_dataset=test_ds,
    compute_metrics=compute_metrics
)

trainer2.train()

pred = trainer2.predict(test_ds)

y_pred = np.argmax(pred.predictions, axis=1)
y_true = pred.label_ids   # ← test_ds 내부의 labels를 그대로 사용
print(confusion_matrix(y_true, y_pred))
print(classification_report(y_true, y_pred, target_names=["Negative","Positive"]))

"""# 라벨링 고도화"""

# --- 1. 전체 데이터 예측 (Inference) ---
# df를 바로 predict에 넣으면 에러가 날 수 있으므로 HFDataset으로 변환
full_ds = HFDataset.from_pandas(df[['content',"sentiment_label","score",'at']]) # 'content'는 리뷰 텍스트 컬럼명
full_ds = full_ds.map(tokenize, batched=True)

all_preds = trainer2.predict(full_ds)
all_probs = softmax(torch.tensor(all_preds.predictions), dim=1).numpy()

# --- 2. 재라벨링 로직 (df 반영) ---
# 모델의 확신도(Softmax 확률) 추출
df['neg_conf'] = all_probs[:, 0]
df['pos_conf'] = all_probs[:, 1]

# 새로운 라벨 결정 (기본은 모델의 argmax)
df['new_label'] = np.argmax(all_probs, axis=1)

# # [선택사항] 만약 기존 별점(sentiment_label)을 존중하면서 0.8 이상일 때만 뒤집고 싶다면:
# threshold = 0.8
# final_labels = df['sentiment_label'].values.copy()
# final_labels[(df['sentiment_label'] == 0) & (df['pos_conf'] >= threshold)] = 1
# final_labels[(df['sentiment_label'] == 1) & (df['neg_conf'] >= threshold)] = 0
# df['new_label'] = final_labels


# 전체 라벨링 결과 저장 (원본 데이터 df 저장)
# 기존 코드를 전혀 수정하지 않고, 마지막에 이 코드만 붙여넣으시면 됩니다.
output_filename = "수정)배달의민족_라벨_고도화.csv"

# 분석에 필요한 핵심 컬럼만 추려서 저장 (기존 df를 건드리지 않음)
df.to_csv(output_filename, index=False, encoding='utf-8-sig')

print(f"전체 라벨링 결과가 '{output_filename}' 파일로 저장되었습니다.")